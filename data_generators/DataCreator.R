#This does the burdensome analysis needed to run Latent Semantic Analysis on the set of MoJ PQs.
#The set itself can be generated by scraping from the web using the "MoJScraper.R" script.




#This code returns an index of PQ documents of decreasing similarity given
#a search query.
#
#It creates a searchSpace.rda file which is used by global.R and server.R.
#


#This creates and saves the following R objects, so that they can be loaded in to an R script

# normalised LSA space to assess similarity with search query (saved as searchSpace.rda)

#Note that searchSpace.rda is used by global.R and server.R

#It also saves two .csv files:
# All the data for PQs, and the cluster that each belongs to (MoJAllPQsForTableau.csv)
# The top dozen words for each cluster (topDozen.csv)

#CAVEAT: To make the eventual similarity query run fast,
#Andy 'sparsified' the normalised rank-reduced
#lsa space by setting all values within 0.01 of zero to zero and putting it into
#simple-triplet format (which doesn't store zero values in memory).  The result
#is that |d| is no longer constant across all documents (ie they are no longer
#quite normalised to length 1).
#They now vary slightly in length, but only differ by 0.036 at most
#(summary stats on the distribution of lengths can be found in the code)
#so it's not too much of a problem.

#To use, just write in the file name of the .csv containing the PQs (which may have been generated
#by MoJScraper.R) under the file parameter below, and run the code.

#LIBRARIES#
library(tm)
library(lsa)
library(cluster)
library(dplyr)
library(slam)
library(stringr)
library(optparse)

#GRAB COMMAND LINE ARGS

option_list = list(
  make_option(c("-i", "--input_file"),
    type    = "character",
    default = str_interp("${SHINY_ROOT}/tests/testthat/examples/data/lsa_training_sample.csv"), 
    help    = "dataset file name",
    metavar = "character"
  ),
  make_option(c("-k", "--k_clusters"),
    type    = "numeric",
    default = 100, 
    help    = "number of clusters [default= %default]",
    metavar = "character"
  ),
  make_option(c("-o", "--output_dir"),
    type    = "character",
    default = str_interp("${SHINY_ROOT}/tests/testthat/examples/data/"), 
    help    = "directory to which outputs are saved [default= %default]",
    metavar = "character"
  )
)
 
opt_parser = OptionParser(option_list=option_list);
opt = parse_args(opt_parser);

#PARAMETERS

# The source file for questions should be passed as a command line argument
# E.g. execute like this -> Rscript DataCreator.R MoJPQsNew.csv

file <- opt$input_file

#FUNCTIONS

#a function to clean a corpus of text, making sure of the encoding, removing punctuation, putting it
#all in lower case, stripping white space, and removing stopwords.
#If you update this you also need to update queryVec in global.R to be in line with any changes, so
#that we are consistent in how we are treating search text and PQ text.
cleanCorpus <- function(corp) {
  corp <- corp %>%
    tm_map(
      content_transformer(
        function(x) iconv(x, to = "utf-8", sub = ""))
    ) %>%
    #inelegant special cleaning step to take care of the fact that reoffending is
    #sometimes spelled "re-offending" and sometimes "reoffending"
    tm_map(function(x) gsub("re-off", "reoff", x)) %>%
    #replace hyphens with spaces
    tm_map(function(x) gsub("-", " ", x)) %>%
    #get rid of all other non-alphanumeric symbols
    tm_map(function(x) gsub("[^(A-Z a-z 0-9 //s)]", "", x)) %>%
    #we now remove Justice with a capital J here before the transformation to lower
    #case, because this deals with the fact that a lot of questions start with "To ask
    #the Secretary of State for Justice" without losing potential information about eg
    #access to justice related questions
    tm_map(function(x) removeWords(x, c("Justice"))) %>%
    tm_map(content_transformer(tolower)) %>%
    #now replace instances of the word "probation" with "probatn" to avoid the
    #issue with "probate" and "probation" being stemmed to the same thing.
    tm_map(content_transformer(
      function(x) gsub("probation", "probatn", x))
    ) %>%
    # JUSTICE_STOP_WORDS assigned in .Rprofile
    tm_map(function(x) removeWords(x, c(stopwords(), JUSTICE_STOP_WORDS))) %>%
    tm_map(stripWhitespace)
}

#a function useful in debugging so you can read a given document in a given corpus easily
writeDoc <- function(num, corpus){
  writeLines(as.character(corpus$content[[num]]))
}

#this will help us unstem words for summary
fromItoY <- function(word){
  return(gsub("i$", "y", word))
}

#a function to summarise the top terms of a given cluster or for a given MP
summarise <- function(type = "cluster", #type can be either cluster or MP
                      ID, #this is the cluster number if type == cluster, or the MPs name in "Surname, Forename" format if type == MP
                      matr, #the tdm as a matrix
                      data, #a hierarchy if type is cluster, or a list of answer MPs if type is MP
                      numTerms, #how many terms to return
                      listOfVectors, #the questions themselves
                      totalClusters = NULL #the number of clusters if type is cluster 
                      ){
  if (type == "cluster"){
    set <- cutree(data, totalClusters)
  } else if (type == "MP"){
    set <- data
  }
  relevantQs <- matr[, which(set == ID)]
  clusterDict <- cleanCorpus(Corpus(VectorSource(listOfVectors[which(set == ID)])))
  termsAndSums <- if (is.null(dim(relevantQs))){
                    relevantQs
                  } else rowSums(relevantQs)
  termsAndSumsN <- termsAndSums[order(termsAndSums, decreasing = T)[1:numTerms]]
  
  #we now complete the word stems, using the fromItoY function to deal with occasions
  #where the unstemming produces blanks
  partialCompletion <- stemCompletion(names(termsAndSumsN), clusterDict)
  toFix <- which(partialCompletion == "")
  fixed <- sapply(names(partialCompletion[toFix]), fromItoY)
  partialCompletion[toFix] <- fixed
  names(termsAndSumsN) <- partialCompletion # update names
  #replace "probatn" with "probation"
  names(termsAndSumsN) <- gsub("probatn", "probation", names(termsAndSumsN))
  #replace "probabl" with "probability"
  names(termsAndSumsN) <- gsub("probabl", "probability", names(termsAndSumsN))
  
  termsAndSumsN
}

#This gets the length of a vector
normVec <- function(vec){
             return(sqrt(sum(vec^2)))
           }

#This normalises the lengths of a matrix to length 1
normalize <- function(mat){
  col.lengths <- sapply(1:ncol(mat), function(x) sqrt(sum(mat[, x]^2)))
  return(sweep(mat, 2, col.lengths, "/"))
}

#This cleans up the names of those asking the questions
nameCleaner <- function(name){
  #first remove surplus white space (I'm looking at you, "Richard  Arkless")
  name <- stripWhitespace(name)
  #first take out Mr/Mrs/Ms
  name <- name %>% gsub("Mr |Mrs |Ms |Miss ","",.)
  #we aim to get everyone's name in the format
  #"surname, {title} firstname {initials}"
  #get the first occurrence of a space
  firstSpace <- regexpr(" ", name) %>% as.vector()
  #use this to get what is usually first name
  firstname <- substr(name, 1, firstSpace - 1)
  #first deal with special cases from the Lords'
  if (firstname == "Lord"|
      firstname == "Lady"|
      firstname == "The"|
      firstname == "Baroness"|
      firstname == "Baron"|
      firstname == "Viscount"){
    #just keep them as Lord/Lady Blah of Blahchester
  }
  #now special cases where someone has a middle initial,
  #or a title like Sir or Dr that we don't want to chop
  else if(length(unlist(gregexpr(" ",name))) > 1){
    #we find the last space, and call everything before it the
    #first name, and every after it the surname
    lastSpace <- unlist(gregexpr(" ", name))[length(unlist(gregexpr(" ",name)))]
    firstname <- substr(name, 1, lastSpace - 1)
    surname <- substr(name, lastSpace + 1, nchar(name))
    name <- paste(surname, firstname, sep = ", ")
  }
  else {
    surname <- substr(name, firstSpace + 1, nchar(name))
    name <- paste(surname, firstname, sep = ", ")
  }
  #now a series of horrible inelegant special cases
  #covering issues like MPs being in the list twice
  #or whatever
  if (name == "Bayley, Hugh"){
    name <- "Bayley, Sir Hugh"
  }
  else if (name == "Blackman-Woods, Roberta"){
    name <- "Blackman-Woods, Dr Roberta"
  }
  else if (name == "Burns, Simon"){
    name <- "Burns, Sir Simon"
  }
  else if (name == "Lucas, Ian C."){
    name <- "Lucas, Ian"
  }
  else if (name == "Morris, Grahame M."){
    name <- "Morris, Grahame"
  }
  else if (name == "Piero, Gloria De"){
    name <- "De Piero, Gloria"
  }
  else if (name == "Bois, Nick de"){
    name <- "de Bois, Nick"
  }
  else if (name == "Soames, Nicholas"){
    name <- "Soames, Sir Nicholas"
  }
  name
}

#PARAMETERS
#Number of clusters, and also rank of LSA space

k <- opt$k_clusters
print(str_interp('K has been set to ${k}'))

#SCRIPT

#GETTING DATA

#read in questions
print('Reading questions')
aPQ <- read.csv(file, stringsAsFactors = F)
questionsVec <- aPQ$Question_Text

#MAKE THE TERM-DOCUMENT MATRIX AND LATENT SEMANTIC ANALYSIS SPACE
print('Making the TDM')
#Create the corpus
PQCorp <- Corpus(VectorSource(questionsVec))
#Stem the corpus
PQCorp.stems <- tm_map(cleanCorpus(PQCorp), stemDocument)

#Create the term-document matrix. For each term in each document we assign a score based on the
#inverse frequency of the appearance of that term in documents in the corpus, normalised for the
#document length (in some sense), and zero if the term is absent from the document entirely.
#Details can be seen by inspecting the help documentation for the weightSMART function.
tdm <- TermDocumentMatrix(
         PQCorp.stems,
         control = list(
           weighting = function(x) weightSMART(x, spec = "btc")))

#Create the latent semantic space. The idea is that it creates a basis of variation, like a PCA, and
#allows you to cut down the number of dimensions you need. 
print('Making the LSA')
lsaAll <- lsa(tdm, dims = dimcalc_raw())

####FIX MP NAMES####
questionerNames <- sapply(aPQ$Question_MP,nameCleaner)
answererNames <- sapply(aPQ$Answer_MP, nameCleaner)

#CLUSTERING
print('Doing some clustering')
#We reduce the LSA space to rank k, and then get the positions of our documents in this latent semantic space.
posns <- diag(lsaAll$sk[1:k]) %*% t(lsaAll$dk[, 1:k])

#distances between documents in this space, based on cosine similarity.
#first normalise columns so all have length 1
normposns <- normalize(posns)
#then since a . b = |a| |b| cos(theta) and we have normalised so that |a| = |b| = 1
#we can just get cos(theta) = a.b
#so we just need to do the dot product between each document
diss <- 1 - (t(normposns) %*% normposns)
#we now take out anything 'small' to make sure things that should be 0 aren't
#rendered tiny and non-zero by floating point arithmetic
diss[which(abs(diss) < 10^-14)] <- 0

#a hierarchical clustering. At the moment we only use this to define our clusters,
#by taking a cut through it at the right stage.
hier <- hclust(as.dist(diss), method = "complete")

#We choose k to be the number of clusters into which we divide our set of questions.
#See the appendix for some sort of reasoning behind this.

klusters <- cutree(hier, k)

m <- as.matrix(tdm)

print("summarising topics")
#this summarises the top 12 terms per cluster using the summarise function from above.
topDozenWordsPerTopic <- data.frame(
  topic = unlist(lapply(seq(1, k), function(x)rep(x, 12))),
  word = unlist(lapply(seq(1, k),
           function(x) names(summarise("cluster", x, m, hier, 12, questionsVec, k)))),
  freq = unlist(lapply(seq(1, k),
           function(x) summarise("cluster", x, m, hier, 12, questionsVec, k))),
  row.names = NULL, stringsAsFactors = F)

#keywords to describe clusters
clusterKeywords <- sapply(seq(k),
                     function(x)
                       names(summarise("cluster", x, m, hier, 3, questionsVec, k)))
clusterKeywordsVec <- sapply(seq_along(clusterKeywords[1, ]),
                        function(x)
                          paste0(clusterKeywords[, x], collapse = ", "))

#Member summaries
print("summarising members")

allMembers <- sort(unique(questionerNames))

topDozenWordsPerMember <- data.frame(
  member = unlist(lapply(allMembers, function(x)rep(x, 12))),
  word = unlist(lapply(allMembers,
                       function(x) names(summarise("MP", x, m, questionerNames, 12, questionsVec)))),
  freq = unlist(lapply(allMembers,
                       function(x) summarise("MP", x, m, questionerNames, 12, questionsVec))),
  row.names = NULL, stringsAsFactors = F)


#MAKE SPACE FOR FAST QUERY SEARCHING
print('Making the search space')
#We reduce the dimensionality of the space to be of rank k, where k is our
#parameter above (also the number of clusters we are going to use)
lsaOut <- lsaAll$tk[, 1:k] %*% posns
#normalise the space
search.space <- normalize(lsaOut)
#"sparsify" by setting all near-zero terms to zero
search.space[which(abs(search.space) < 0.01)] <- 0

##This is just a check to see that this sparsification doesn't lead to wildly varying document lengths
collengths <- sapply(seq_along(aPQ$Question_ID),
                function(x) normVec(search.space[, x]))
summary(collengths)

#save disk space by saving as simple triplet matrix
search.space <- as.simple_triplet_matrix(search.space)



#### SAVING ####
print('Saving the output')
#save(tdm, file = "tdm.rda")
#save(lsaOut,file = "lsaOut.rda")
#save(klusters,file = "klusters.rda")

save_location = opt$output_dir
setwd(save_location)

save(search.space, file = "searchSpace.rda")

#Save data to be directly loaded in to Tableau

#The questions and their data (including cluster)
savedf <- data.frame(
  Document_Number = seq_along(aPQ$Question_ID),
  Question_ID = aPQ$Question_ID,
  Question_Text = aPQ$Question_Text,
  Answer_Text = aPQ$Answer_Text,
  Question_MP = questionerNames,
  MP_Constituency = aPQ$MP_Constituency,
  Answer_MP = answererNames,
  Date = aPQ$Question_Date,
  Answer_Date = aPQ$Answer_Date,
  #Corrected_Date = aPQ$Corrected_Date,
  Topic = klusters,
  Topic_Keywords = clusterKeywordsVec[klusters],
  stringsAsFactors = FALSE)
write.csv(savedf, "MoJwrittenPQs.csv")

#The information about the clusters
write.csv(topDozenWordsPerTopic, "topDozenWordsPerTopic.csv")

#The information about the members
write.csv(topDozenWordsPerMember, "topDozenWordsPerMember.csv")

##### APPENDIX #####

#Here we see how many clusters is a good number for our data. We calculate the
#silhouette for each clustering - the higher the better. We also calculate the
#median number of questions per cluster given the total cluster number.
#If these calculations have already been done you can simply load the
#"silhouettewidths.rda" and "medianpercluster.rda" files. Otherwise you will
#have to regenerate the value running the code.

#load(file = "silhouettewidths.rda")
#load(file = "medianpercluster.rda")
#load(file = "clusterings.rda")

#if you want to regenerate the data run the following

#minClust <- 2
#maxClust <- 4000
#clusterings <- sapply(seq(minClust, maxClust),
#                 function(x) cutree(hier,x))

#kSilWidths <- sapply(seq(minClust, maxClust),
#                function(x)
#                  mean(
#                    silhouette(
#                      clusterings[, x + 1 - minClust], diss)[, 3]
#                  )
#               )
#names(kSilWidths) <- seq(minClust, maxClust)

#medianClusterMembership <- sapply(seq(minClust, maxClust),
#                             function(x){
#                               median(
#                                 sapply(
#                                   seq(minClust, x),
#                                     function(y) length(which(clusterings[, x + 1 - minClust] == y))
#                                 )
#                               )
#                             }
#                            )

#if you want to save it
#save(kSilWidths, file = "silhouettewidths.rda")
#save(medianClusterMembership, file = "medianpercluster.rda")
#save(clusterings, file = "clusterings.rda")

#plot(kSilWidths, type = "l")
#which.max(kSilWidths)
#max(kSilWidths)
#medianClusterMembership[which.max(kSilWidths)]
#you can see that the "best" number of clusters is around 2668. However, this results in
#a median of only two questions per cluster, and the silhouette is still pretty small, at ~0.23.
#So we probably want more questions per cluster on average, particularly as it's not like the
#clusterings are "good" anyway. Hence the arbitrary choice of 1000, which gives a silhouette
#of ~0.161 and a median of 4 questions per cluster.
#kSilWidths[2000]
#medianClusterMembership[2000]

#We might be able to do better than arbitrarily picking 1000 by defining some function of
#median and silhouette and maximising it (although then the function definition is still
#arbitary).
