#This does the burdensome analysis needed to run Latent Semantic Analysis on the set of MoJ PQs.
#The set itself can be generated by scraping from the web using the 'MoJScraper.R' script.

#This creates and saves the following R objects, so that they can be loaded in to an R script
#run in the background for Tableau:
# Latent Semantic Analysis space (saved as lsaOut.rda)
# Text-Document Matrix (saved as tdm.rda)
# Results of a clustering (saved as kluster.rda)

#It also saves two .csv files which are then directly loaded into Tableau as data:
# All the data for PQs, and the cluster that each belongs to (MoJAllPQsForTableau.csv)
# The top dozen words for each cluster (topDozen.csv)

#To use, just write in the file name of the .csv containing the PQs (which may have been generated
#by MoJScraper.R) under the file parameter below, and run the code.

#LIBRARIES#
library(tm)
library(lsa)
library(cluster)
library(LSAfun)

#PARAMETERS

file <- 'MoJPQsNew.csv'

#FUNCTIONS

#a list of stopwords to be removed from the PQs to avoid false similarities on the grounds
#of questions containing these words
stopwordList <- c(
  stopwords(),'a','b','c','d','i','ii','iii','iv',
  'secretary','state','ministry','majesty',
  'government','many','ask','whether',
  'assessment','further','pursuant','justice',
  'minister','steps','department','question'
)

#a function to clean a corpus of text, making sure of the encoding, removing punctuation, putting it
#all in lower case, stripping white space, and removing stopwords.
cleanCorpus <- function(corp) {
  corp <-tm_map(corp, content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub='byte')))
  toSpace <- content_transformer(function(x, pattern) { return (gsub(pattern, ' ', x))})
  corp <- tm_map(corp, toSpace, '-')
  corp <- tm_map(corp, toSpace, '’')
  corp <- tm_map(corp, toSpace, '‘')
  corp <- tm_map(corp, toSpace, '•')
  corp <- tm_map(corp, toSpace, '”')
  corp <- tm_map(corp, toSpace, '“')
  corp <- tm_map(corp,content_transformer(tolower))
  corp <- tm_map(corp,removePunctuation)
  corp <- tm_map(corp,stripWhitespace)
  corp <- tm_map(corp, function(x) removeWords(x,stopwordList))
}

#a function useful in debugging so you can read a given document in a given corpus easily
writeDoc <- function(num,corpus){
  writeLines(as.character(corpus$content[[num]]))
}

#a function to summarise the top terms of a given cluster
summarise <- function(clusterNum,matr,totalClusters,hierarchy,numTerms,listOfVectors){
  clusterSet <- cutree(hierarchy,totalClusters)
  relevantQs <- matr[,which(clusterSet==clusterNum)]
  clusterDict <- cleanCorpus(Corpus(VectorSource(listOfVectors[which(clusterSet==clusterNum)])))
  termsAndSums <- if(is.null(dim(relevantQs))){relevantQs} else rowSums(relevantQs)
  termsAndSumsN <- termsAndSums[order(termsAndSums,decreasing=T)[1:numTerms]]
  names(termsAndSumsN) <- stemCompletion(names(termsAndSumsN),clusterDict)
  termsAndSumsN
}


#SCRIPT

#read in questions
aPQ <- read.csv(file,stringsAsFactors = F)
questionsVec <- aPQ$Question_Text

#make sure it's in utf-8 format
questionsVec <- iconv(questionsVec,to="utf-8-mac")

#Create the corpus
PQCorp <- Corpus(VectorSource(questionsVec))
#Stem the corpus
PQCorp.stems <- tm_map(cleanCorpus(PQCorp),stemDocument)

#Create the term-document matrix. For each term in each document we assign a score based on the
#inverse frequency of the appearance of that term in documents in the corpus, normalised for the
#document length (in some sense), and zero if the term is absent from the document entirely.
#Details can be seen by inspecting the help documentation for the weightSMART function.
tdm<-TermDocumentMatrix(PQCorp.stems,control =list(weighting = function(x) weightSMART(x, spec = "btc")))

#Creat the latent semantic space. The idea is that it creates a basis of variation, like a PCA, and
#allows you to cut down the number of dimensions you need. Here I've determined the number of dimensions
#such that all of them contribute an s-value of at least 1 (the 'Kaiser-Criterion').
lsaOut <- lsa(tdm,dims=dimcalc_kaiser())
#positions of our documents in this latent semantic space.
posns <-diag(lsaOut$sk) %*% t(lsaOut$dk)
#distances between documents in this space, based on cosine similarity.
diss <- 1-cosine(posns)
#a hierarchical clustering. At the moment we only use this to define our clusters,
#by taking a cut through it at the right stage. There is no doubt more that could
#be done using the hierarchy.
hier<-hclust(as.dist(diss),method = "complete")

#We choose 1000 to be the number of clusters into which we divide our set of questions.
#See the appendix for some sort of reasoning behind this.
k <- 1000
klusters <- cutree(hier,k)
#this summarises the top 12 terms per cluster using the summarise function from above.
topDozen <- data.frame(
  cluster=unlist(lapply(seq(1,k),function(x)rep(x,12))),
  word=unlist(lapply(seq(1,k),function(x) names(summarise(x,m,k,hier,12,questionsVec)))),
  freq=unlist(lapply(seq(1,k),function(x) summarise(x,m,k,hier,12,questionsVec))),
  row.names = NULL ,stringsAsFactors = F)


#### SAVING ####

#Save the R output to be loaded in to R when Tableau is running
save(tdm, file='tdm.rda')
save(lsaOut,file='lsaOut.rda')
save(klusters,file='klusters.rda')

#Save data to be directly loaded in to Tableau

#The questions and their data (including cluster)
savedf <- data.frame(
  Question_ID = aPQ$Question_ID,
  Question_Text = aPQ$Question_Text,
  Answer_Text = aPQ$Answer_Text,
  Question_MP = aPQ$Question_MP,
  MP_Constituency = aPQ$MP_Constituency,
  Answer_MP = aPQ$Answer_MP,
  Date = aPQ$Date,
  Answer_Date = aPQ$Answer_Date,
  Corrected_Date = aPQ$Corrected_Date,
  Cluster = klusters,
  stringsAsFactors = FALSE)
write.csv(savedf,'MoJallPQsforTableau.csv')

#The information about the clusters
write.csv(topDozen,'topDozen.csv')


##### APPENDIX #####

#Here we see how many clusters is a good number for our data. We calculate the
#silhouette for each clustering - the higher the better. We also calculate the
#median number of questions per cluster given the total cluster number.
#If these calculations have already been done you can simply load the
#'silhouettewidths.rda' and 'medianpercluster.rda' files. Otherwise you will
#have to regenerate the value running the code.

load(file='silhouettewidths.rda')
load(file='medianpercluster.rda')

#if you want to regenerate the data run the following
#ksilwidths <- sapply(seq(2,4000), function(x) mean(silhouette(cutree(hier,x),distn)[,3]))
#if you want to save it
#save(ksilwidths,file='silhouettewidths.rda')
#medianNumPerCluster <- function(hierarch,k){
#  klusters <- cutree(hierarch,k)
#  median(sapply(seq(1,k), function(x) length(which(klusters == x))))
#}
#meds <- sapply(seq(2,4000),function(x) medianNumPerCluster(hier,x))
#save(meds,file='medianpercluster.rda')

plot(ksilwidths, type="l")
which.max(ksilwidths)
max(ksilwidths)
meds[which.max(ksilwidths)]
#you can see that the 'best' number of clusters is around 2668. However, this results in
#a median of only two questions per cluster, and the silhouette is still pretty small, at ~0.23.
#So we probably want more questions per cluster on average, particularly as it's not like the
#clusterings are 'good' anyway. Hence the arbitrary choice of 1000, which gives a silhouette
#of ~0.161 and a median of 4 questions per cluster.
ksilwidths[1000]
meds[1000]

#We might be able to do better than arbitrarily picking 1000 by defining some function of
#median and silhouette and maximising it (although then the function definition is still
#arbitary).
